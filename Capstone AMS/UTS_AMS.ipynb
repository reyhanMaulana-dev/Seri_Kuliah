{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5djFduOrK0MOzIEHFi+Pp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reyhanMaulana-dev/Seri_Kuliah/blob/main/Capstone%20AMS/UTS_AMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reyhan Maulana A (11220940000048)\n",
        "\n",
        "Hoirorun nisa (11220940000003)\n",
        "\n",
        "Windy Tri Oktaviani (11220940000027)\n",
        "\n",
        "Muhammad Ikhwan Farhat (11210940000007)"
      ],
      "metadata": {
        "id": "rrB9Ll4UtvIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library yang dipakai\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ItFyzfW_XfM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. Preprocessing"
      ],
      "metadata": {
        "id": "FKNfBDsX_hn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1vDlvX9VTjNbHrrtae4NHSKYGG8QuKpe0/export?format=csv')\n",
        "df"
      ],
      "metadata": {
        "id": "5tfQEg3W_hwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Casefolding"
      ],
      "metadata": {
        "id": "gEbW2qqvX6YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Casefolding ->ubah teks komentar jadi huruf kecil\n",
        "def casefolding(komentar):\n",
        "    komentar = komentar.lower()\n",
        "    return komentar\n",
        "\n",
        "#note :sebelum tokenizing text harus masuk tahap cleansing dan casefolding dulu, agar tidak berpengaruh pada proses perhitungnan pada algoritm dan hasil konsisten"
      ],
      "metadata": {
        "id": "ibEOcTkv_h0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Cleaning Text"
      ],
      "metadata": {
        "id": "E7CO29yvX_-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat Fungsi-fungsi untuk text preprocessing\n",
        "#1. fungsi untuk cleansing teks\n",
        "import re #import library re / Regular expression (regex)\n",
        "\n",
        "#a. fungsi Cleaning\n",
        "def cleaning(komentar):\n",
        "  komentar = re.sub(r\"http\\S+\",'',komentar) #hapus link -->udah bisa tapi sisa \"a hrep\"\n",
        "  komentar = re.sub(r\"<a href=\",'',komentar) #hapus tag <a href\n",
        "  komentar = re.sub(r\"</a>\",'',komentar) #hapus tag </a>\n",
        "  komentar = re.sub(r\"<br>\",' ',komentar) #hapus tag <br> baris baru\n",
        "  komentar = re.sub(r\"&quot\",'',komentar) #hapus tag &quot\n",
        "  komentar = re.sub(r\"&amp\",'',komentar) #hapus tag &amp\n",
        "  komentar = re.sub(r'[0-9]+',' ',komentar) #hapus angka\n",
        "  komentar = re.sub(r\"[-()\\\"#/@$%*;&:<>^{}'+=~|.!?,_]\", \" \", komentar) #hapus tanda baca (punctuation)\n",
        "  komentar = komentar.strip(' ') #hapus whitespace --work\n",
        "  komentar = komentar.encode('ascii', 'ignore').decode('ascii') #hapus emoticon\n",
        "  return komentar"
      ],
      "metadata": {
        "id": "90Q_oZaY_h4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b. hapus karakter sama lebih dari 2 contoh josss\tmenjadi jos\n",
        "def replaceTOM(komentar):\n",
        "    pola = re.compile(r'(.)\\1{2,}', re.DOTALL)\n",
        "    return pola.sub(r'\\1', komentar)\n"
      ],
      "metadata": {
        "id": "SjPIEh2U_h7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Tokenizing"
      ],
      "metadata": {
        "id": "YW7u-gKHYrgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing 'punkt_tab' data\n",
        "\n",
        "#fungsi tokenisasi\n",
        "def tokenizingText(komentar):\n",
        "    # Check if the input is a string\n",
        "    if isinstance(komentar, str):\n",
        "        komentar = word_tokenize(komentar)\n",
        "    # If not a string (likely a float representing NaN), return an empty list\n",
        "    else:\n",
        "        komentar = []\n",
        "    return komentar"
      ],
      "metadata": {
        "id": "PMbolp2fYr0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Normalisasi SLANGWORD\n",
        "Tahap Normalisasi dilakukan untuk pengubah penggunaan kata slang/gaul menjadi baku. Proses akan menggunakan file dataset slangwords yang berisi kata slang yang nanti akan diubah menjadi baku. Tahap ini dibantu dengan library RegEx"
      ],
      "metadata": {
        "id": "Gwga9CQ0YsGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdwon\n",
        "import gdown\n",
        "\n",
        "file_id = \"1zpE-eRz8n8Iawcwqq5EkOEkhYpjGf8QM\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output=\"slangwords.txt\", quiet=False)"
      ],
      "metadata": {
        "id": "0-jyCtpSZLhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToSlangword(ulasan):\n",
        "    with open('slangwords.txt', 'r') as file:\n",
        "        kamusSlang = eval(file.read())\n",
        "\n",
        "    hasil = []\n",
        "    for kata in ulasan:\n",
        "        kata_baku = kamusSlang.get(kata.lower(), kata)  # ganti jika ada di kamus\n",
        "        hasil.append(kata_baku.lower())\n",
        "    return hasil"
      ],
      "metadata": {
        "id": "Z4w6ASBXY-kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Stopword Removal (filtering)\n",
        "Hapus kata-kata tidak penting"
      ],
      "metadata": {
        "id": "JD6eJihJe9Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "vReNbfVvfBS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "daftar_stopword = stopwords.words('indonesian')\n",
        "# ---------------------------- manualy add stopword  ------------------------------------\n",
        "# append additional stopword\n",
        "daftar_stopword.extend([\"yg\",\"dg\",\"cuy\",\"bang\",\"bng\",\"densu\",\"gtu\",\"walsh\",\"ke\",\"sty\",\"ole\",\"jordi\",\"amat\",\"romeny\",\"nya\",\"yang\",\"ii\",\n",
        "    'yang', 'tidak', 'di', 'dan', 'ini',\n",
        "    'saja', 'kalau', 'itu', 'ada', 'dari', 'sudah', 'kamu', 'bisa', 'liga',\n",
        "    'jadi', 'juga', 'tapi', 'iya', 'lib', 'bukan', 'pt', 'sama', 'ke', 'untuk', 'nya',\n",
        "    'ingin', 'erickthohir', 'dengan', 'saya', 'idho', 'karena', 'bola', 'dia', 'masih',\n",
        "    'idextratime', 'buat', 'klub', 'orang', 'apa', 'lagi', 'kita', 'banyak', 'mereka',\n",
        "    'lebih', 'atau', 'sih', 'seperti', 'tahun', 'main', 'negara', 'id',\n",
        "    'sampai', 'lokal', 'harus', 'tim', 'cuma', 'malaysia', 'akan', 'semua', 'gk', 'yg', 'tua',\n",
        "    'lah', 'indo', 'piala', 'sekarang', 'dulu', 'pada', 'terus',\n",
        "    'persib', 'punya', 'baru', 'masuk', 'bahkan', 'pakai',\n",
        "    'jangan', 'perlu', 'belum', 'bagus', 'banget', 'mah', 'emang', 'mungkin', 'dunia',\n",
        "    'unmagnetism', 'oghiegigs', 'biar', 'dalam', 'tahu', 'sepak', 'satu', 'pak', 'wasit', 'et', 'u'\n",
        "]) #tambahan stopwordnya\n",
        "daftar_stopword = set(daftar_stopword)\n",
        "\n",
        "def stopwordText(words):\n",
        " return [word for word in words if word not in daftar_stopword]\n"
      ],
      "metadata": {
        "id": "llxc-mrxfDpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.Stemming\n",
        "Stemming adalah proses pengubahan bentuk kata menjadi kata dasar atau tahap mencari root dari tiap kata. menghapus sufix, prefix dkk"
      ],
      "metadata": {
        "id": "wY5GGQ2zhOL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library Sastrawi (Stemming Nazief-Adriani) pada proses Stemming:\n",
        "# Download librari sastawwi\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Inisialisasi stemmer (cukup sekali saja)\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Fungsi stemming\n",
        "def stemming(komentar):\n",
        "    # Handle non-string/non-list types like NaN (float) by returning an empty list\n",
        "    if not isinstance(komentar, (list, str)):\n",
        "        return []\n",
        "    # Pastikan komentar berupa string, bukan list\n",
        "    if isinstance(komentar, list):\n",
        "        komentar = ' '.join(komentar)  # gabungkan jika masih list token\n",
        "    hasil = stemmer.stem(komentar)     # stemming kalimat\n",
        "    return hasil.split()               # pisahkan kembali jadi list token"
      ],
      "metadata": {
        "id": "5UUGQeHXhW3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply preprocessing"
      ],
      "metadata": {
        "id": "wEcxAcoNYsJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def toString(text):\n",
        "  return \" \".join(text)\n"
      ],
      "metadata": {
        "id": "8FfibPOiY-7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepocessing(text):\n",
        "\n",
        "  # Pipeline preprocessing\n",
        "  text = casefolding(text)\n",
        "  text = cleaning(text)\n",
        "  text = replaceTOM(text)\n",
        "  text = tokenizingText(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "RkGCmtRoY_AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['textDisplay'] = df['textDisplay'].apply(prepocessing)\n",
        "df['textDisplay'] = df['textDisplay'].apply(convertToSlangword)\n",
        "df['textDisplay'] = df['textDisplay'].apply(stopwordText) #apply fungsi stopword removal\n",
        "df['textDisplay'] = df['textDisplay'].apply(stemming)\n",
        "df['textDisplay'] = df['textDisplay'].apply(toString)"
      ],
      "metadata": {
        "id": "Jr085zeoYsMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "21OqvxIPYsPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving data"
      ],
      "metadata": {
        "id": "GLOrSWDEg9zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"datafinal_ams.csv\",index = False, header = True,index_label=None)\n"
      ],
      "metadata": {
        "id": "grLNts6hhDDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. EDA"
      ],
      "metadata": {
        "id": "IR0mBr8gkN5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "XaZQlF2SrQki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/13mvXwCQF6mT61B_4O_mHlL_FNGZKj0k4Oj2JT6Qeb7s/export?format=csv')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "CaUJVfrjkQ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "FvucCYBvlG0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
        "df.dropna(how='any', inplace=True)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "WrR82CIklKC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "oD5m_ZMFlgdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribusi Media Sosial"
      ],
      "metadata": {
        "id": "qR54NET7ZWBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Atur style visualisasi\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Visualisasi 1: Total komentar per platform\n",
        "plt.figure(figsize=(8, 5))\n",
        "platform_counts = df['platfrom'].value_counts()\n",
        "sns.barplot(x=platform_counts.index, y=platform_counts.values, palette=\"Blues_d\")\n",
        "plt.title(\"Total Komentar per Platform\")\n",
        "plt.xlabel(\"Platform\")\n",
        "plt.ylabel(\"Jumlah Komentar\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zPDC-OdfqRqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribusi Sentimen"
      ],
      "metadata": {
        "id": "6ussU0zlkSGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sentiment_counts = df['label'].value_counts()\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"Set2\")\n",
        "plt.title(\"Distribusi Sentimen\")\n",
        "plt.xlabel(\"Label Sentimen\")\n",
        "plt.ylabel(\"Jumlah Komentar\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n1Z6wtQ6qTE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Berdasarkan Waktu"
      ],
      "metadata": {
        "id": "b08k939QkgQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def cek_periode(tanggal):\n",
        "\n",
        "    if tanggal.year == 2024 and tanggal.month <= 12:\n",
        "        return \"2024\"\n",
        "    elif tanggal.year == 2025 and tanggal.month <= 2:\n",
        "        return \"2025 Awal\"\n",
        "    elif tanggal.year == 2025 and tanggal.month <= 5:\n",
        "        return \"2025 Pertengahan\"\n",
        "    else:\n",
        "        return \"Di luar rentang\"\n",
        "\n",
        "df['publishedAt'] = df['publishedAt'].apply(cek_periode)"
      ],
      "metadata": {
        "id": "9r7R3XUYklaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 =df[['publishedAt','platfrom','label']].copy()\n",
        "data_grouped = df1.groupby(['publishedAt', 'label']).size().reset_index(name='jumlah_komentar')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=data_grouped, x='publishedAt', y='jumlah_komentar', hue='label')\n",
        "plt.title(\"Jumlah Komentar per Periode berdasarkan Sentimen\")\n",
        "plt.xlabel(\"Periode Waktu\")\n",
        "plt.ylabel(\"Jumlah Komentar\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ExCHLX9kmV3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "sns.catplot(\n",
        "    data=df,\n",
        "    kind=\"count\",\n",
        "    x=\"publishedAt\",\n",
        "    hue=\"label\",\n",
        "    col=\"platfrom\",\n",
        "    palette=\"Set2\",\n",
        "    height=4,\n",
        "    aspect=1\n",
        ")\n",
        "plt.subplots_adjust(top=0.8)\n",
        "plt.suptitle(\"Distribusi Komentar per Periode per Platform dan Sentimen\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hjL9wnDnozxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCLoud"
      ],
      "metadata": {
        "id": "m-TjwRts4ya0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "df2 = df[['textDisplay','label', 'platfrom']].copy()\n",
        "df2 = df2[(df2['platfrom']==\"Instagram\") & (df2['label']==\"positif\")]\n",
        "\n",
        "# Gabungkan semua teks dari kolom textDisplay, ubah ke lowercase\n",
        "all_text = ' '.join(df2['textDisplay'].dropna().astype(str)).lower()\n",
        "\n",
        "# Tokenisasi: ambil kata (alfanumerik) saja\n",
        "tokens = re.findall(r'\\b\\w+\\b', all_text)\n",
        "\n",
        "# Hitung frekuensi kata\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Buang kata yang tidak diinginkan\n",
        "exclude_words = {'program', 'makan', 'gratis','sekolah','anak'}\n",
        "for word in exclude_words:\n",
        "    word_freq.pop(word, None)\n",
        "\n",
        "# Filter hanya kata yang muncul ≥ 10 kali\n",
        "filtered_freq = {word: count for word, count in word_freq.items() if count >= 10}\n",
        "\n",
        "# Buat WordCloud dengan colormap merah\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update(['program', 'makan', 'gratis','bpjs','prabowo'])\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='black',\n",
        "    colormap='Blues',\n",
        "    max_font_size=100,\n",
        "    min_font_size=10,\n",
        "    stopwords=stopwords,\n",
        "    contour_color='black',\n",
        "    contour_width=1,\n",
        "    prefer_horizontal=0.9\n",
        ").generate_from_frequencies(filtered_freq)\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Wordcloud Sentimen Positif\", fontsize=18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9tgy8fgAqRuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "df2 = df[['textDisplay','label', 'platfrom']].copy()\n",
        "df2 = df2[(df2['platfrom']==\"Instagram\") & (df2['label']==\"negatif\")]\n",
        "\n",
        "# Gabungkan semua teks dari kolom textDisplay, ubah ke lowercase\n",
        "all_text = ' '.join(df2['textDisplay'].dropna().astype(str)).lower()\n",
        "\n",
        "# Tokenisasi: ambil kata (alfanumerik) saja\n",
        "tokens = re.findall(r'\\b\\w+\\b', all_text)\n",
        "\n",
        "# Hitung frekuensi kata\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Buang kata yang tidak diinginkan\n",
        "exclude_words = {'program', 'makan', 'gratis','sekolah','anak'}\n",
        "for word in exclude_words:\n",
        "    word_freq.pop(word, None)\n",
        "\n",
        "# Filter hanya kata yang muncul ≥ 10 kali\n",
        "filtered_freq = {word: count for word, count in word_freq.items() if count >= 10}\n",
        "\n",
        "# Buat WordCloud dengan colormap merah\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update(['program', 'makan', 'gratis','bpjs','prabowo'])\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='black',\n",
        "    colormap='Reds',\n",
        "    max_font_size=100,\n",
        "    min_font_size=10,\n",
        "    stopwords=stopwords,\n",
        "    contour_color='black',\n",
        "    contour_width=1,\n",
        "    prefer_horizontal=0.9\n",
        ").generate_from_frequencies(filtered_freq)\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Wordcloud Sentimen Negatif\", fontsize=18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yy-G3gZlrQ5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}